{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTerrier\n",
    "\n",
    "_IN4325: Information retrieval lecture, TU Delft_\n",
    "\n",
    "**Part 7: Neural ranking models**\n",
    "\n",
    "In this part, we'll learn how to use **neural ranking models** with PyTerrier. Since PyTerrier itself does not implement any neural rankers, we'll use the [**Fast-Forward indexes** library](https://github.com/mrjleo/fast-forward-indexes), which focuses on efficient re-ranking using neural models and provides PyTerrier integration (using transformers). In order to learn more about Fast-Forward indexes, have a look at the [documentation](https://mrjleo.github.io/fast-forward-indexes/docs/v0.2.0/) and the [corresponding paper](https://dl.acm.org/doi/abs/10.1145/3485447.3511955).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: python-terrier==0.10.0 in /home/anistor/.local/lib/python3.10/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: fast-forward-indexes==0.2.0 in /home/anistor/.local/lib/python3.10/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: numpy in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (2.2.2)\r\n",
      "Requirement already satisfied: wget in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (3.2)\r\n",
      "Requirement already satisfied: tqdm in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (4.66.2)\r\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.6.1)\r\n",
      "Requirement already satisfied: matchpy in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.5)\r\n",
      "Requirement already satisfied: scikit-learn in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.4.2)\r\n",
      "Requirement already satisfied: deprecated in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.2.14)\r\n",
      "Requirement already satisfied: chest in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.2.3)\r\n",
      "Requirement already satisfied: scipy in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.13.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from python-terrier==0.10.0) (2.31.0)\r\n",
      "Requirement already satisfied: joblib in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.4.0)\r\n",
      "Requirement already satisfied: nptyping==1.4.4 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.4.4)\r\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from python-terrier==0.10.0) (8.10.0)\r\n",
      "Requirement already satisfied: ir-datasets>=0.3.2 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.6)\r\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from python-terrier==0.10.0) (3.0.3)\r\n",
      "Requirement already satisfied: statsmodels in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.14.2)\r\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.3.3)\r\n",
      "Requirement already satisfied: dill in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.3.8)\r\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.6)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/anistor/.local/lib/python3.10/site-packages (from fast-forward-indexes==0.2.0) (2.2.2)\r\n",
      "Requirement already satisfied: transformers>=4.28.0 in /home/anistor/.local/lib/python3.10/site-packages (from fast-forward-indexes==0.2.0) (4.40.0)\r\n",
      "Requirement already satisfied: h5py>=3.0.0 in /home/anistor/.local/lib/python3.10/site-packages (from fast-forward-indexes==0.2.0) (3.11.0)\r\n",
      "Requirement already satisfied: typish>=1.7.0 in /home/anistor/.local/lib/python3.10/site-packages (from nptyping==1.4.4->python-terrier==0.10.0) (1.9.3)\r\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (4.12.3)\r\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (2.5.0)\r\n",
      "Requirement already satisfied: lxml>=4.5.2 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (5.2.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (5.4.1)\r\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (2.6)\r\n",
      "Requirement already satisfied: lz4>=3.1.10 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (4.3.3)\r\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.3)\r\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.5)\r\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.1.6)\r\n",
      "Requirement already satisfied: ijson>=3.1.3 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (3.2.3)\r\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.1.12)\r\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.2)\r\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /home/anistor/.local/lib/python3.10/site-packages (from ir-measures>=0.3.1->python-terrier==0.10.0) (1.0.12)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->python-terrier==0.10.0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->python-terrier==0.10.0) (2022.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/anistor/.local/lib/python3.10/site-packages (from pandas->python-terrier==0.10.0) (2024.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier==0.10.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->python-terrier==0.10.0) (3.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->python-terrier==0.10.0) (1.26.5)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->python-terrier==0.10.0) (2020.6.20)\r\n",
      "Requirement already satisfied: filelock in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (3.13.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (1.12)\r\n",
      "Requirement already satisfied: networkx in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (3.3)\r\n",
      "Requirement already satisfied: fsspec in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (2024.3.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /home/anistor/.local/lib/python3.10/site-packages (from torch>=2.0.0->fast-forward-indexes==0.2.0) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/anistor/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->fast-forward-indexes==0.2.0) (12.4.127)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/anistor/.local/lib/python3.10/site-packages (from transformers>=4.28.0->fast-forward-indexes==0.2.0) (0.22.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.0->fast-forward-indexes==0.2.0) (24.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/anistor/.local/lib/python3.10/site-packages (from transformers>=4.28.0->fast-forward-indexes==0.2.0) (2024.4.16)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/anistor/.local/lib/python3.10/site-packages (from transformers>=4.28.0->fast-forward-indexes==0.2.0) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/anistor/.local/lib/python3.10/site-packages (from transformers>=4.28.0->fast-forward-indexes==0.2.0) (0.4.3)\r\n",
      "Requirement already satisfied: heapdict in /home/anistor/.local/lib/python3.10/site-packages (from chest->python-terrier==0.10.0) (1.0.1)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/anistor/.local/lib/python3.10/site-packages (from deprecated->python-terrier==0.10.0) (1.16.0)\r\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /home/anistor/.local/lib/python3.10/site-packages (from matchpy->python-terrier==0.10.0) (2.1.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/anistor/.local/lib/python3.10/site-packages (from scikit-learn->python-terrier==0.10.0) (3.4.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/anistor/.local/lib/python3.10/site-packages (from statsmodels->python-terrier==0.10.0) (0.5.6)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier==0.10.0) (2.5)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5.6->statsmodels->python-terrier==0.10.0) (1.16.0)\r\n",
      "Requirement already satisfied: cbor>=1.0.0 in /home/anistor/.local/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier==0.10.0) (1.0.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/anistor/.local/lib/python3.10/site-packages (from sympy->torch>=2.0.0->fast-forward-indexes==0.2.0) (1.3.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-terrier==0.10.0 fast-forward-indexes==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: In order to execute this notebook in reasonable time, you'll need a CUDA-capable GPU. If you have one, follow the [official tutorials](https://pytorch.org/get-started/locally/) and install PyTorch with CUDA acceleration. If you do not have one, you can use Google Colab: Under \"Edit\" -> \"Notebook settings\" -> \"Hardware accelerator\", select a GPU.\n",
    "\n",
    "If the installation was successful, the following should return `True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Side note_: In this notebook, we focus on the **retrieve-and-re-rank** setting. PyTerrier supports **dense retrieval** models through plugins (such as [pyterrier_ance](https://github.com/terrierteam/pyterrier_ance)). Since dense retrieval is often very resource-demanding, we do not cover it here. Another library that provides many pre-trained models and dense retrieval indexes is [pyserini](https://github.com/castorini/pyserini).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "\n",
    "    pt.init(tqdm=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the [FiQA dataset](https://sites.google.com/view/fiqa/home), which is a dataset for financial question answering. Since QA pipelines usually include a retrieval/ranking step, FiQA provides a corpus, queries (questions), and corresponding QRels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"irds:beir/fiqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the domain of this dataset (finance), the questions and documents are relatively complex. We'll see how much a large neural re-ranking model (based on BERT) manages to improve over term matching (BM25).\n",
    "\n",
    "Let's create a lexical index first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "beir/fiqa documents:   0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e049ba56efd6471692b9cd0300cab0be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "indexer = pt.IterDictIndexer(\n",
    "    str(Path.cwd()),  # this will be ignored\n",
    "    type=pt.index.IndexingType.MEMORY,\n",
    ")\n",
    "index_ref = indexer.index(dataset.get_corpus_iter(), fields=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our baseline, we measure BM25 performance without any re-ranking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "              name     RR@10   nDCG@10   AP@100\n0  Cache(BR(BM25))  0.310271  0.252589  0.20864",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@10</th>\n      <th>AP@100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cache(BR(BM25))</td>\n      <td>0.310271</td>\n      <td>0.252589</td>\n      <td>0.20864</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import RR, nDCG, MAP\n",
    "\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")\n",
    "testset = pt.get_dataset(\"irds:beir/fiqa/test\")\n",
    "pt.Experiment(\n",
    "    [~bm25],\n",
    "    testset.get_topics(),\n",
    "    testset.get_qrels(),\n",
    "    eval_metrics=[RR @ 10, nDCG @ 10, MAP @ 100],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast-Forward indexes\n",
    "\n",
    "Fast-forward indexes use _dual-encoder models_ (the same that are used in dense retrieval) for _interpolation-based re-ranking_. The benefit of this (compared to cross-encoders) is that document representations only need to be computed once (during the indexing step) and can be looked up during re-ranking.\n",
    "\n",
    "### The encoders\n",
    "\n",
    "We'll start by instantiating the encoders. [TCT-ColBERT](https://github.com/castorini/tct_colbert) is a single-vector dual-encoder model based on BERT, where the query and document encoders are identical (Siamese architecure). A pre-trained model (trained on MS MARCO) is [available on the Hugging Face hub](https://huggingface.co/castorini/tct_colbert-msmarco). We'll use this model in a transfer setting (i.e., without fine-tuning) on the FiQA dataset.\n",
    "\n",
    "The encoders can be loaded as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.encoder import TCTColBERTQueryEncoder, TCTColBERTDocumentEncoder\n",
    "import torch\n",
    "\n",
    "q_encoder = TCTColBERTQueryEncoder(\"castorini/tct_colbert-msmarco\")\n",
    "d_encoder = TCTColBERTDocumentEncoder(\n",
    "    \"castorini/tct_colbert-msmarco\",\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these encoders to compute vector representations of text pieces, such as queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.105638  ,  0.02958424,  0.13244788, ...,  0.02808921,\n        -0.00611512,  0.0177914 ],\n       [-0.16003011,  0.02995185,  0.19217151, ..., -0.02004313,\n        -0.02355196, -0.019374  ]], dtype=float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_encoder([\"query 1\", \"query 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The index\n",
    "\n",
    "We've already created an index for the BM25 retriever earlier. For the dense vector representations, we'll need to create another separate index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File ffindex_fiqa_tct.h5 exists.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfast_forward\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OnDiskIndex, Mode\n\u001B[0;32m----> 3\u001B[0m ff_index \u001B[38;5;241m=\u001B[39m \u001B[43mOnDiskIndex\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mffindex_fiqa_tct.h5\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m768\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_encoder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mq_encoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMAXP\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/fast_forward/index/disk.py:60\u001B[0m, in \u001B[0;36mOnDiskIndex.__init__\u001B[0;34m(self, index_file, dim, query_encoder, mode, encoder_batch_size, init_size, resize_min_val, hdf5_chunk_size, dtype, max_id_length, overwrite, ds_buffer_size)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Create an index.\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    ValueError: When the file exists and `overwrite=False`.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index_file\u001B[38;5;241m.\u001B[39mexists() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m overwrite:\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m exists.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(query_encoder, mode, encoder_batch_size)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_file \u001B[38;5;241m=\u001B[39m index_file\u001B[38;5;241m.\u001B[39mabsolute()\n",
      "\u001B[0;31mValueError\u001B[0m: File ffindex_fiqa_tct.h5 exists."
     ]
    }
   ],
   "source": [
    "from fast_forward import OnDiskIndex, Mode\n",
    "\n",
    "ff_index = OnDiskIndex(\n",
    "    Path(\"ffindex_fiqa_tct.h5\"), dim=768, query_encoder=q_encoder, mode=Mode.MAXP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set `dim=768`, because our encoders output `768`-dimensional representations. `Mode.MAXP` determines how documents that have multiple vectors are scored; however, since our documents only have a single representation each, this will have no effect; we just need to set this to tell the index we're working with documents and not passages (more on that [here](https://mrjleo.github.io/fast-forward-indexes/docs/v0.2.0/fast_forward/index.html#ranking-mode)).\n",
    "\n",
    "Now we can index the corpus using our document encoder. This is done using the `Indexer` utility class. We'll use an iterator that simply yields the documents in the correct format.\n",
    "\n",
    "**The next cell will take a while to execute, even with GPU acceleration.** You can adjust the batch size according to your available VRAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward import Indexer\n",
    "\n",
    "\n",
    "def docs_iter():\n",
    "    for d in dataset.get_corpus_iter():\n",
    "        yield {\"doc_id\": d[\"docno\"], \"text\": d[\"text\"]}\n",
    "\n",
    "\n",
    "ff_indexer = Indexer(ff_index, d_encoder, batch_size=8)\n",
    "ff_indexer.index_dicts(docs_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Once indexing is done, we can always load the index instead of indexing everything again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:00<00:00, 386564.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from fast_forward import OnDiskIndex, Mode\n",
    "\n",
    "ff_index = OnDiskIndex.load(\n",
    "    Path(\"ffindex_fiqa_tct.h5\"), query_encoder=q_encoder, mode=Mode.MAXP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, if you have enough RAM, you can load the entire index (i.e., all vector representations) into the main memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_index = ff_index.to_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking BM25 results\n",
    "\n",
    "We've already performed re-ranking in the learning-to-rank notebook. In order to use a Fast-Forward index for re-ranking, we wrap it in an `FFScore` transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.util.pyterrier import FFScore\n",
    "\n",
    "ff_score = FFScore(ff_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how this works, we take the queries from the testset and retrieve a small number of candidate documents for each one using BM25:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "         qid  docid   docno  rank      score  \\\n0       4641  36224  376148     0  41.677305   \n1       4641  47916  497993     1  29.149791   \n2       4641  55690  580025     2  26.773005   \n3       4641  24501  253614     3  26.640181   \n4       4641   3157   32833     4  24.265187   \n...      ...    ...     ...   ...        ...   \n644153  2399  33136  343489     0  33.280064   \n644154  2399   6704   69171     1  31.335596   \n644155  2399   4148   43046     2  31.265964   \n644156  2399  46670  484891     3  29.869008   \n644157  2399  13154  135220     4  28.911571   \n\n                                                    query  \n0         where should i park my rainy day emergency fund  \n1         where should i park my rainy day emergency fund  \n2         where should i park my rainy day emergency fund  \n3         where should i park my rainy day emergency fund  \n4         where should i park my rainy day emergency fund  \n...                                                   ...  \n644153  where do web sites get foreign exchange curren...  \n644154  where do web sites get foreign exchange curren...  \n644155  where do web sites get foreign exchange curren...  \n644156  where do web sites get foreign exchange curren...  \n644157  where do web sites get foreign exchange curren...  \n\n[3240 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>docid</th>\n      <th>docno</th>\n      <th>rank</th>\n      <th>score</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4641</td>\n      <td>36224</td>\n      <td>376148</td>\n      <td>0</td>\n      <td>41.677305</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4641</td>\n      <td>47916</td>\n      <td>497993</td>\n      <td>1</td>\n      <td>29.149791</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4641</td>\n      <td>55690</td>\n      <td>580025</td>\n      <td>2</td>\n      <td>26.773005</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4641</td>\n      <td>24501</td>\n      <td>253614</td>\n      <td>3</td>\n      <td>26.640181</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4641</td>\n      <td>3157</td>\n      <td>32833</td>\n      <td>4</td>\n      <td>24.265187</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>644153</th>\n      <td>2399</td>\n      <td>33136</td>\n      <td>343489</td>\n      <td>0</td>\n      <td>33.280064</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n    <tr>\n      <th>644154</th>\n      <td>2399</td>\n      <td>6704</td>\n      <td>69171</td>\n      <td>1</td>\n      <td>31.335596</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n    <tr>\n      <th>644155</th>\n      <td>2399</td>\n      <td>4148</td>\n      <td>43046</td>\n      <td>2</td>\n      <td>31.265964</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n    <tr>\n      <th>644156</th>\n      <td>2399</td>\n      <td>46670</td>\n      <td>484891</td>\n      <td>3</td>\n      <td>29.869008</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n    <tr>\n      <th>644157</th>\n      <td>2399</td>\n      <td>13154</td>\n      <td>135220</td>\n      <td>4</td>\n      <td>28.911571</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3240 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = (~bm25 % 5)(testset.get_topics())\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, this data frame contains a score for each query-document pair. We can apply our new re-ranking transformer to the candidates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       qid   docno    score_0      score  \\\n0     4641  376148  41.677305  67.637367   \n1     4641  497993  29.149791  68.335365   \n2     4641  580025  26.773005  67.524490   \n3     4641  253614  26.640181  67.098091   \n4     4641   32833  24.265187  68.345680   \n...    ...     ...        ...        ...   \n3235  2399  343489  33.280064  69.723114   \n3236  2399   69171  31.335596  67.643097   \n3237  2399   43046  31.265964  67.293716   \n3238  2399  484891  29.869008  68.099777   \n3239  2399  135220  28.911571  68.129173   \n\n                                                  query  \n0       where should i park my rainy day emergency fund  \n1       where should i park my rainy day emergency fund  \n2       where should i park my rainy day emergency fund  \n3       where should i park my rainy day emergency fund  \n4       where should i park my rainy day emergency fund  \n...                                                 ...  \n3235  where do web sites get foreign exchange curren...  \n3236  where do web sites get foreign exchange curren...  \n3237  where do web sites get foreign exchange curren...  \n3238  where do web sites get foreign exchange curren...  \n3239  where do web sites get foreign exchange curren...  \n\n[3240 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>docno</th>\n      <th>score_0</th>\n      <th>score</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4641</td>\n      <td>376148</td>\n      <td>41.677305</td>\n      <td>67.637367</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4641</td>\n      <td>497993</td>\n      <td>29.149791</td>\n      <td>68.335365</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4641</td>\n      <td>580025</td>\n      <td>26.773005</td>\n      <td>67.524490</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4641</td>\n      <td>253614</td>\n      <td>26.640181</td>\n      <td>67.098091</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4641</td>\n      <td>32833</td>\n      <td>24.265187</td>\n      <td>68.345680</td>\n      <td>where should i park my rainy day emergency fund</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3235</th>\n      <td>2399</td>\n      <td>343489</td>\n      <td>33.280064</td>\n      <td>69.723114</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n    <tr>\n      <th>3236</th>\n      <td>2399</td>\n      <td>69171</td>\n      <td>31.335596</td>\n      <td>67.643097</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n    <tr>\n      <th>3237</th>\n      <td>2399</td>\n      <td>43046</td>\n      <td>31.265964</td>\n      <td>67.293716</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n    <tr>\n      <th>3238</th>\n      <td>2399</td>\n      <td>484891</td>\n      <td>29.869008</td>\n      <td>68.099777</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n    <tr>\n      <th>3239</th>\n      <td>2399</td>\n      <td>135220</td>\n      <td>28.911571</td>\n      <td>68.129173</td>\n      <td>where do web sites get foreign exchange curren...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3240 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_ranked = ff_score(candidates)\n",
    "re_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `score` column has now been updated to reflect the re-ranking scores. Furthermore, there is a new column, `score_0`, which contains the original retrieval scores. As mentioned earlier, Fast-Forward indexes focus on _interpolation-based re-ranking_. In essence, the idea is to take both lexical retrieval scores $s_{\\text{lex}}$ and semantic re-ranking scores $s_{\\text{sem}}$ into account, such that the final score $s$ is computed as follows:\n",
    "\n",
    "$$s = \\alpha s_{\\text{lex}} + (1-\\alpha) s_{\\text{sem}}$$\n",
    "\n",
    "We can perform the interpolation using the `FFInterpolate` transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       qid   docno                                              query  \\\n0     4641  376148    where should i park my rainy day emergency fund   \n1     4641  497993    where should i park my rainy day emergency fund   \n2     4641  580025    where should i park my rainy day emergency fund   \n3     4641  253614    where should i park my rainy day emergency fund   \n4     4641   32833    where should i park my rainy day emergency fund   \n...    ...     ...                                                ...   \n3235  2399  343489  where do web sites get foreign exchange curren...   \n3236  2399   69171  where do web sites get foreign exchange curren...   \n3237  2399   43046  where do web sites get foreign exchange curren...   \n3238  2399  484891  where do web sites get foreign exchange curren...   \n3239  2399  135220  where do web sites get foreign exchange curren...   \n\n          score  \n0     54.657336  \n1     48.742578  \n2     47.148748  \n3     46.869136  \n4     46.305434  \n...         ...  \n3235  51.501589  \n3236  49.489347  \n3237  49.279840  \n3238  48.984392  \n3239  48.520372  \n\n[3240 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>docno</th>\n      <th>query</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4641</td>\n      <td>376148</td>\n      <td>where should i park my rainy day emergency fund</td>\n      <td>54.657336</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4641</td>\n      <td>497993</td>\n      <td>where should i park my rainy day emergency fund</td>\n      <td>48.742578</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4641</td>\n      <td>580025</td>\n      <td>where should i park my rainy day emergency fund</td>\n      <td>47.148748</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4641</td>\n      <td>253614</td>\n      <td>where should i park my rainy day emergency fund</td>\n      <td>46.869136</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4641</td>\n      <td>32833</td>\n      <td>where should i park my rainy day emergency fund</td>\n      <td>46.305434</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3235</th>\n      <td>2399</td>\n      <td>343489</td>\n      <td>where do web sites get foreign exchange curren...</td>\n      <td>51.501589</td>\n    </tr>\n    <tr>\n      <th>3236</th>\n      <td>2399</td>\n      <td>69171</td>\n      <td>where do web sites get foreign exchange curren...</td>\n      <td>49.489347</td>\n    </tr>\n    <tr>\n      <th>3237</th>\n      <td>2399</td>\n      <td>43046</td>\n      <td>where do web sites get foreign exchange curren...</td>\n      <td>49.279840</td>\n    </tr>\n    <tr>\n      <th>3238</th>\n      <td>2399</td>\n      <td>484891</td>\n      <td>where do web sites get foreign exchange curren...</td>\n      <td>48.984392</td>\n    </tr>\n    <tr>\n      <th>3239</th>\n      <td>2399</td>\n      <td>135220</td>\n      <td>where do web sites get foreign exchange curren...</td>\n      <td>48.520372</td>\n    </tr>\n  </tbody>\n</table>\n<p>3240 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fast_forward.util.pyterrier import FFInterpolate\n",
    "\n",
    "ff_int = FFInterpolate(alpha=0.5)\n",
    "ff_int(re_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data frame above, both scores have been fused into one with equal weights.\n",
    "\n",
    "Now we're ready to take everything for a spin. Let's compare our re-ranker to BM25:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "         name     RR@10   nDCG@10    AP@100\n0        BM25  0.310271  0.252589  0.208640\n1  BM25 >> FF  0.334666  0.274198  0.224551",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@10</th>\n      <th>AP@100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BM25</td>\n      <td>0.310271</td>\n      <td>0.252589</td>\n      <td>0.208640</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BM25 &gt;&gt; FF</td>\n      <td>0.334666</td>\n      <td>0.274198</td>\n      <td>0.224551</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [~bm25, ~bm25 % 100 >> ff_score >> ff_int],\n",
    "    testset.get_topics(),\n",
    "    testset.get_qrels(),\n",
    "    eval_metrics=[RR @ 10, nDCG @ 10, MAP @ 100],\n",
    "    names=[\"BM25\", \"BM25 >> FF\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the performance already improved quite nicely. But thinking back, we chose $\\alpha=0.5$ pretty much arbitrarily. How do we know this is really the best value?\n",
    "\n",
    "### Validation\n",
    "\n",
    "PyTerrier offers several functions to determine the best hyperparameters for a ranker. In the following, we'll use [`pyterrier.GridSearch`](https://pyterrier.readthedocs.io/en/latest/tuning.html#pyterrier.GridSearch) to find the best value for $\\alpha$.\n",
    "\n",
    "**Important**: When you tune hyperparameters of your model, **do not use the same data you use for testing (i.e., the testset)**. Otherwise, your results are invalid, because you optimized your method for the testing data. Instead, we'll use the development (validation) data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "devset = pt.get_dataset(\"irds:beir/fiqa/dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTerriers `GridSearch` class can be used to automatically run an experiment multiple times in order to find the hyperparameters that result in the best performance.\n",
    "\n",
    "We'll use a similar pipeline as before, but we limit the number of candidate documents to `100` in order to reduce the runtime. We provide a list of values for `alpha` and a metric (MAP), which is used to decide which value results in the best performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pt.GridSearch(\n",
    "    ~bm25 % 100 >> ff_score >> ff_int,\n",
    "    {ff_int: {\"alpha\": [0.05, 0.1, 0.5, 0.9]}},\n",
    "    devset.get_topics(),\n",
    "    devset.get_qrels(),\n",
    "    \"map\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, out of the options we provided, `alpha=0.1` was the best.\n",
    "\n",
    "_Side note_: As of now, PyTerrier does not support caching for re-ranking transformers. Hence, `GridSearch` takes a long time, because the scores are re-computed every time, even though that wouldn't be necessary.\n",
    "\n",
    "Conveniently, the best value has already been set for us in the transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ff_int.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat our experiment on the testset with the optimal hyperparameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pt.Experiment(\n",
    "    [~bm25, ~bm25 % 1000 >> ff_score >> ff_int],\n",
    "    testset.get_topics(),\n",
    "    testset.get_qrels(),\n",
    "    eval_metrics=[RR @ 10, nDCG @ 10, MAP @ 100],\n",
    "    names=[\"BM25\", \"BM25 >> FF\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the value of `alpha` makes a big difference!\n",
    "\n",
    "_Final remarks_: We've used neural models for re-ranking only in this notebook. However, in practice, it is not uncommon to use the re-ranking scores we computed here as a feature for a learning-to-rank model.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "Check out the [section on neural models](https://pyterrier.readthedocs.io/en/latest/neural.html) in the PyTerrier documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ranking')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e757c603b03bb5364a2e7c54e488a14c8c5b6d905c56042b6aec9e13d3e2c7b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
