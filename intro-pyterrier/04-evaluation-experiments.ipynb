{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTerrier\n",
    "\n",
    "_IN4325: Information retrieval lecture, TU Delft_\n",
    "\n",
    "**Part 4: Evaluation & experiments**\n",
    "\n",
    "This part focuses on running experiments and evaluating retrieval and ranking models using PyTerrier. We'll learn how to use query relevances (QRels) in order to compute ranking metrics and perform statistical significance testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: python-terrier==0.10.0 in /home/anistor/.local/lib/python3.10/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: numpy in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (2.2.2)\r\n",
      "Requirement already satisfied: wget in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (3.2)\r\n",
      "Requirement already satisfied: tqdm in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (4.66.2)\r\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.6.1)\r\n",
      "Requirement already satisfied: matchpy in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.5)\r\n",
      "Requirement already satisfied: scikit-learn in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.4.2)\r\n",
      "Requirement already satisfied: deprecated in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.2.14)\r\n",
      "Requirement already satisfied: chest in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.2.3)\r\n",
      "Requirement already satisfied: scipy in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.13.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from python-terrier==0.10.0) (2.31.0)\r\n",
      "Requirement already satisfied: joblib in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.4.0)\r\n",
      "Requirement already satisfied: nptyping==1.4.4 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.4.4)\r\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from python-terrier==0.10.0) (8.10.0)\r\n",
      "Requirement already satisfied: ir-datasets>=0.3.2 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.6)\r\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from python-terrier==0.10.0) (3.0.3)\r\n",
      "Requirement already satisfied: statsmodels in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.14.2)\r\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.3.3)\r\n",
      "Requirement already satisfied: dill in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.3.8)\r\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /home/anistor/.local/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.6)\r\n",
      "Requirement already satisfied: typish>=1.7.0 in /home/anistor/.local/lib/python3.10/site-packages (from nptyping==1.4.4->python-terrier==0.10.0) (1.9.3)\r\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (4.12.3)\r\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (2.5.0)\r\n",
      "Requirement already satisfied: lxml>=4.5.2 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (5.2.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (5.4.1)\r\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (2.6)\r\n",
      "Requirement already satisfied: lz4>=3.1.10 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (4.3.3)\r\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.3)\r\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.5)\r\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.1.6)\r\n",
      "Requirement already satisfied: ijson>=3.1.3 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (3.2.3)\r\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.1.12)\r\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /home/anistor/.local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.2)\r\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /home/anistor/.local/lib/python3.10/site-packages (from ir-measures>=0.3.1->python-terrier==0.10.0) (1.0.12)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier==0.10.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->python-terrier==0.10.0) (3.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->python-terrier==0.10.0) (1.26.5)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->python-terrier==0.10.0) (2020.6.20)\r\n",
      "Requirement already satisfied: heapdict in /home/anistor/.local/lib/python3.10/site-packages (from chest->python-terrier==0.10.0) (1.0.1)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/anistor/.local/lib/python3.10/site-packages (from deprecated->python-terrier==0.10.0) (1.16.0)\r\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /home/anistor/.local/lib/python3.10/site-packages (from matchpy->python-terrier==0.10.0) (2.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->python-terrier==0.10.0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->python-terrier==0.10.0) (2022.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/anistor/.local/lib/python3.10/site-packages (from pandas->python-terrier==0.10.0) (2024.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/anistor/.local/lib/python3.10/site-packages (from scikit-learn->python-terrier==0.10.0) (3.4.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/anistor/.local/lib/python3.10/site-packages (from statsmodels->python-terrier==0.10.0) (0.5.6)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier==0.10.0) (24.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier==0.10.0) (2.5)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5.6->statsmodels->python-terrier==0.10.0) (1.16.0)\r\n",
      "Requirement already satisfied: cbor>=1.0.0 in /home/anistor/.local/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier==0.10.0) (1.0.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-terrier==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(tqdm=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries and query relevance\n",
    "\n",
    "Let's start by loading an existing dataset. We'll pick one from [the list](https://pyterrier.readthedocs.io/en/latest/datasets.html#available-datasets) that has topics (queries) and QRels available, such as `vaswani`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"vaswani\")\n",
    "index = dataset.get_index(variant=\"terrier_stemmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the queries (topics) in this dataset. Note that this dataset only has one set of queries and QRels (you can see this in the table). If a dataset has multiple sets, you'll need to specify one, for example: `dataset.get_topics(\"test\")`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   qid                                              query\n0    1  measurement of dielectric constant of liquids ...\n1    2  mathematical analysis and design details of wa...\n2    3  use of digital computers in the design of band...\n3    4    systems of data coding for information transfer\n4    5  use of programs in engineering testing of comp...\n..  ..                                                ...\n88  89  tunnel diode construction and its electrical c...\n89  90  electronic density of states at the surface of...\n90  91  resistivity of metallic thin films related to ...\n91  92  the phenomenon of radiation caused by charged ...\n92  93  high frequency oscillators using transistors t...\n\n[93 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>measurement of dielectric constant of liquids ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>mathematical analysis and design details of wa...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>use of digital computers in the design of band...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>systems of data coding for information transfer</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>use of programs in engineering testing of comp...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>89</td>\n      <td>tunnel diode construction and its electrical c...</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>90</td>\n      <td>electronic density of states at the surface of...</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>91</td>\n      <td>resistivity of metallic thin films related to ...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>92</td>\n      <td>the phenomenon of radiation caused by charged ...</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>93</td>\n      <td>high frequency oscillators using transistors t...</td>\n    </tr>\n  </tbody>\n</table>\n<p>93 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the set of queries, we can inspect the corresponding relevance data (QRels). These are tuples of (query, document, label). Note that labels can be either binary (`0` or `1`) or graded (for example, `0` to `3`, where a higher number encodes higher relevance). Tuples where the label is `0` (i.e., no relevance) are often omitted from the QRels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     qid  docno  label\n0      1   1239      1\n1      1   1502      1\n2      1   4462      1\n3      1   4569      1\n4      1   5472      1\n...   ..    ...    ...\n2078  93   9875      1\n2079  93   9956      1\n2080  93  10497      1\n2081  93  11191      1\n2082  93  11318      1\n\n[2083 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>docno</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1239</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1502</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>4462</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4569</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>5472</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2078</th>\n      <td>93</td>\n      <td>9875</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2079</th>\n      <td>93</td>\n      <td>9956</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2080</th>\n      <td>93</td>\n      <td>10497</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2081</th>\n      <td>93</td>\n      <td>11191</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2082</th>\n      <td>93</td>\n      <td>11318</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2083 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_qrels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Using the QRels and the output of a ranking model (i.e., a _ranking_ or a _run_), we can compute ranking metrics that evaluate the quality of our results. Let's start with a BM25 model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An _experiment_ in PyTerrier automates the retrieval/ranking and evaluation process. This means that you can get a comprehensive evaluation of models or systems on some test set in a single line of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       name     RR@10   nDCG@20        AP\n0  BR(BM25)  0.719875  0.415377  0.296517",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@20</th>\n      <th>AP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BR(BM25)</td>\n      <td>0.719875</td>\n      <td>0.415377</td>\n      <td>0.296517</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import RR, nDCG, MAP\n",
    "\n",
    "pt.Experiment(\n",
    "    [bm25],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[RR @ 10, nDCG @ 20, MAP],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have evaluted our BM25 retriever on the dataset we loaded earlier. We used three of the most common IR metrics, but [PyTerrier supports a very large number of additional metrics](https://pyterrier.readthedocs.io/en/latest/experiments.html#evaluation-measures-objects) through its integration of the [ir-measures](https://github.com/terrierteam/ir_measures) library.\n",
    "\n",
    "**Important**: Usually, a value of `0` indicates _no relevance_, while a value greater than `0` indicates _relevance_. However, this is not always the case; a popular example is the TREC Deep Learning passage ranking task, where [only values greater than `1` indicate relevance](https://trec.nist.gov/data/deep2020.html). In those cases, the threshold can by adjusted by constructing the metric as, for example, `RR(rel=2)`.\n",
    "\n",
    "Note that we used the `@` operator to limit the retrieval depth (number of documents per query) for specific metrics. Furthermore, since both queries (topics) and QRels are simply packaged in a `pandas.DataFrame`, you can easily provide your own data, as long as it is in the right format.\n",
    "\n",
    "### Performance comparisons\n",
    "\n",
    "Experiments let you easily compare multiple ranking approaches. Let's load another ranker, which uses standard TF-IDF, to see how much BM25 improves over it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run an experiment that evaluates both models on the same data. For better readability, we assign custom names to both approaches.\n",
    "\n",
    "Comparing the approaches in the same experiment allows us to automatically have _statistical significance testing_ performed. By setting `baseline=0`, we tell the function to compute the $p$-values with respect to the first approach (TF-IDF). Furthermore, [PyTerrier supports a number of correction methods](https://pyterrier.readthedocs.io/en/latest/experiments.html#significance-testing); here, we apply Bonferroni correction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     name        AP   nDCG@20     RR@10  AP +  AP -  AP p-value  AP reject  \\\n0  TF-IDF  0.290905  0.410850  0.694666   NaN   NaN         NaN      False   \n1    BM25  0.296517  0.415377  0.719875  46.0  45.0    0.237317      False   \n\n   AP p-value corrected  nDCG@20 +  nDCG@20 -  nDCG@20 p-value  \\\n0                   NaN        NaN        NaN              NaN   \n1              0.474634       31.0       34.0         0.309461   \n\n   nDCG@20 reject  nDCG@20 p-value corrected  RR@10 +  RR@10 -  RR@10 p-value  \\\n0           False                        NaN      NaN      NaN            NaN   \n1           False                   0.618923      9.0      2.0       0.034943   \n\n   RR@10 reject  RR@10 p-value corrected  \n0         False                      NaN  \n1         False                 0.069886  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>AP</th>\n      <th>nDCG@20</th>\n      <th>RR@10</th>\n      <th>AP +</th>\n      <th>AP -</th>\n      <th>AP p-value</th>\n      <th>AP reject</th>\n      <th>AP p-value corrected</th>\n      <th>nDCG@20 +</th>\n      <th>nDCG@20 -</th>\n      <th>nDCG@20 p-value</th>\n      <th>nDCG@20 reject</th>\n      <th>nDCG@20 p-value corrected</th>\n      <th>RR@10 +</th>\n      <th>RR@10 -</th>\n      <th>RR@10 p-value</th>\n      <th>RR@10 reject</th>\n      <th>RR@10 p-value corrected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TF-IDF</td>\n      <td>0.290905</td>\n      <td>0.410850</td>\n      <td>0.694666</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BM25</td>\n      <td>0.296517</td>\n      <td>0.415377</td>\n      <td>0.719875</td>\n      <td>46.0</td>\n      <td>45.0</td>\n      <td>0.237317</td>\n      <td>False</td>\n      <td>0.474634</td>\n      <td>31.0</td>\n      <td>34.0</td>\n      <td>0.309461</td>\n      <td>False</td>\n      <td>0.618923</td>\n      <td>9.0</td>\n      <td>2.0</td>\n      <td>0.034943</td>\n      <td>False</td>\n      <td>0.069886</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [tf_idf, bm25],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    names=[\"TF-IDF\", \"BM25\"],\n",
    "    eval_metrics=[RR @ 10, nDCG @ 20, MAP],\n",
    "    baseline=0,\n",
    "    correction=\"bonferroni\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results\n",
    "\n",
    "Experiments allow for saving the obtained results (runs) in the common TREC format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     name     RR@10   nDCG@20        AP\n0  TF-IDF  0.694666  0.410850  0.290905\n1    BM25  0.719875  0.415377  0.296517",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@20</th>\n      <th>AP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TF-IDF</td>\n      <td>0.694666</td>\n      <td>0.410850</td>\n      <td>0.290905</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BM25</td>\n      <td>0.719875</td>\n      <td>0.415377</td>\n      <td>0.296517</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "pt.Experiment(\n",
    "    [tf_idf, bm25],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    names=[\"TF-IDF\", \"BM25\"],\n",
    "    eval_metrics=[RR @ 10, nDCG @ 20, MAP],\n",
    "    save_dir=str(results_dir),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful when results need to be shared or processed futher. In addition, PyTerrier will re-use saved results rather than re-computing them. This behavior can be disabled by setting `save_mode=\"overwrite\"`.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "Check out the [section on experiments](https://pyterrier.readthedocs.io/en/latest/experiments.html) in the documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
