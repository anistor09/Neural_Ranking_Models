{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "General Setup for all the datasets( first 3 cells)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "from experiment_utils.experiments_helper import time_fct\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "path_to_root = \"../../\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation metrics used for all the datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from pyterrier.measures import RR, nDCG, MAP\n",
    "\n",
    "eval_metrics = [RR @ 10, nDCG @ 10, MAP @ 100]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the query encoder that will run on CPU. Encoder used for embedding all the datasets/queries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from encoders.snowflake_arctic_embed_m import SnowFlakeQueryEncoder\n",
    "\n",
    "package = \"Snowflake/\"\n",
    "model_name = \"snowflake-arctic-embed-m\"\n",
    "q_encoder = SnowFlakeQueryEncoder(package + model_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NFCorpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following experiments, the alpha tuning is not used and we preset it to 0.05. We only wanted to see how fast each experiment. Timeit library was not used because I wanted to first understand if these are normal running times overall and then assess the latency for each experiment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sparse Retrieval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment took 3.236 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": "             name     RR@10   nDCG@10    AP@100\n0  nfcorpus: BM25  0.534378  0.322219  0.143582",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@10</th>\n      <th>AP@100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nfcorpus: BM25</td>\n      <td>0.534378</td>\n      <td>0.322219</td>\n      <td>0.143582</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_utils.experiments_helper import test_first_stage_retrieval_name\n",
    "\n",
    "dataset_name = \"nfcorpus\"\n",
    "# dev_set_name = \"irds:beir/nfcorpus/dev\"\n",
    "test_set_name = \"irds:beir/nfcorpus/test\"\n",
    "\n",
    "pipeline_name = \"BM25\"\n",
    "\n",
    "time_fct(test_first_stage_retrieval_name, dataset_name, test_set_name, eval_metrics, pipeline_name, path_to_root)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "FFI Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No third element available\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexperiment_utils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperiments_helper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m default_complete_test_pipeline_name\n\u001B[1;32m      3\u001B[0m pipeline_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBM25 >> \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m model_name\n\u001B[0;32m----> 4\u001B[0m \u001B[43mtime_fct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdefault_complete_test_pipeline_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_set_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m         \u001B[49m\u001B[43mq_encoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_metrics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpipeline_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath_to_root\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anistor-Neural-ranking-models/experiment_utils/experiments_helper.py:153\u001B[0m, in \u001B[0;36mtime_fct\u001B[0;34m(func, *args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtime_fct\u001B[39m(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    152\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m--> 153\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m    155\u001B[0m     elapsed_time \u001B[38;5;241m=\u001B[39m end_time \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m~/anistor-Neural-ranking-models/experiment_utils/experiments_helper.py:105\u001B[0m, in \u001B[0;36mdefault_complete_test_pipeline_name\u001B[0;34m(dataset_name, test_set_name, q_encoder, eval_metrics, model_name, pipeline_name, path_to_root, dev_set_name)\u001B[0m\n\u001B[1;32m    102\u001B[0m     dev_set \u001B[38;5;241m=\u001B[39m pt\u001B[38;5;241m.\u001B[39mget_dataset(dev_set_name)\n\u001B[1;32m    103\u001B[0m     dev_topics \u001B[38;5;241m=\u001B[39m dev_set\u001B[38;5;241m.\u001B[39mget_topics()\n\u001B[0;32m--> 105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdefault_complete_test_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_qrels\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_topics\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq_encoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43meval_metrics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpipeline_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath_to_root\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mdev_topics\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anistor-Neural-ranking-models/experiment_utils/experiments_helper.py:116\u001B[0m, in \u001B[0;36mdefault_complete_test_pipeline\u001B[0;34m(dataset_name, qrels, test_topics, q_encoder, eval_metrics, model_name, pipeline_name, path_to_root, dev_topics)\u001B[0m\n\u001B[1;32m    113\u001B[0m retriever \u001B[38;5;241m=\u001B[39m load_sparse_index_from_disk(dataset_name, path_to_root)\n\u001B[1;32m    115\u001B[0m \u001B[38;5;66;03m# Dense index\u001B[39;00m\n\u001B[0;32m--> 116\u001B[0m dense_index \u001B[38;5;241m=\u001B[39m \u001B[43mload_dense_index_from_disk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq_encoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m ff_score \u001B[38;5;241m=\u001B[39m FFScore(dense_index)\n\u001B[1;32m    119\u001B[0m ff_int \u001B[38;5;241m=\u001B[39m FFInterpolate(alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m)\n",
      "File \u001B[0;32m~/anistor-Neural-ranking-models/experiment_utils/experiments_helper.py:32\u001B[0m, in \u001B[0;36mload_dense_index_from_disk\u001B[0;34m(dataset_name, query_encoder, model_name, mode)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_dense_index_from_disk\u001B[39m(dataset_name, query_encoder, model_name, mode\u001B[38;5;241m=\u001B[39mMode\u001B[38;5;241m.\u001B[39mMAXP):\n\u001B[0;32m---> 32\u001B[0m     index_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdense_indexes/ffindex_\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mget_dataset_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m format_name(\n\u001B[1;32m     33\u001B[0m         model_name) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;66;03m# Retrieve from disk\u001B[39;00m\n\u001B[1;32m     37\u001B[0m     ff_index \u001B[38;5;241m=\u001B[39m OnDiskIndex\u001B[38;5;241m.\u001B[39mload(\n\u001B[1;32m     38\u001B[0m         Path(index_path), query_encoder\u001B[38;5;241m=\u001B[39mquery_encoder, mode\u001B[38;5;241m=\u001B[39mmode)\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "from experiment_utils.experiments_helper import default_complete_test_pipeline_name\n",
    "\n",
    "pipeline_name = \"BM25 >> \" + model_name\n",
    "time_fct(default_complete_test_pipeline_name, dataset_name, test_set_name,\n",
    "         q_encoder, eval_metrics, model_name, pipeline_name, path_to_root)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run pipeline for FIQA dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:00<00:00, 822787.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment took 56.576 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                             name     RR@10   nDCG@10    AP@100\n0  fiqa: BM25 >> gte-base-en-v1.5  0.399449  0.335966  0.274557",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@10</th>\n      <th>AP@100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fiqa: BM25 &gt;&gt; gte-base-en-v1.5</td>\n      <td>0.399449</td>\n      <td>0.335966</td>\n      <td>0.274557</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_utils.experiments_helper import default_complete_test_pipeline_name\n",
    "\n",
    "dataset_name = \"fiqa\"\n",
    "# dev_set_name = \"irds:beir/fiqa/dev\"\n",
    "dataset_test_name = \"irds:beir/fiqa/test\"\n",
    "\n",
    "time_fct(\n",
    "    default_complete_test_pipeline_name, dataset_name, dataset_test_name, q_encoder, eval_metrics)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## For the Scidocs dataset, considering the lack of a dev set, the train set was used for finetuning the alpha value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given that the scidocs dataset offers only one dataset, we will split it into dev and test set. More exactly, we will split the topics because that is what we are testing against. I chose the 'text' topics as this dataset offers more topics categories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25657/25657 [00:00<00:00, 679372.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment took 76.129 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                name     RR@10   nDCG@10    AP@100\n0  scidocs: BM25 >> gte-base-en-v1.5  0.292146  0.165924  0.113075",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@10</th>\n      <th>AP@100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>scidocs: BM25 &gt;&gt; gte-base-en-v1.5</td>\n      <td>0.292146</td>\n      <td>0.165924</td>\n      <td>0.113075</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_utils.experiments_helper import split_dev_test, default_complete_test_pipeline\n",
    "\n",
    "dataset_name = \"scidocs\"\n",
    "dataset = pt.get_dataset(\"irds:beir/scidocs\")\n",
    "topics = dataset.get_topics('text')\n",
    "\n",
    "dev_topics, test_topics = split_dev_test(topics, test_size=0.8)\n",
    "\n",
    "time_fct(default_complete_test_pipeline, dataset_name, dataset.get_qrels(), test_topics, q_encoder,\n",
    "         eval_metrics)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A similar approach is also followed for the \"cqadupstack/english\" dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40221/40221 [00:00<00:00, 1227563.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment took 153.739 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                            name     RR@10   nDCG@10    AP@100\n0  cqadupstack/english: BM25 >> gte-base-en-v1.5  0.366865  0.356954  0.326041",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@10</th>\n      <th>AP@100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cqadupstack/english: BM25 &gt;&gt; gte-base-en-v1.5</td>\n      <td>0.366865</td>\n      <td>0.356954</td>\n      <td>0.326041</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_utils.experiments_helper import split_dev_test, default_complete_test_pipeline\n",
    "\n",
    "dataset_name = \"cqadupstack/english\"\n",
    "dataset = pt.get_dataset(\"irds:beir/cqadupstack/english\")\n",
    "topics = dataset.get_topics('text')\n",
    "\n",
    "dev_topics, test_topics = split_dev_test(topics, test_size=0.8)\n",
    "\n",
    "time_fct(default_complete_test_pipeline, dataset_name, dataset.get_qrels(), test_topics, q_encoder, eval_metrics)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A similar approach is also followed for the \"arguana\" dataset.\n",
    "\n",
    "### DelftBlue runtime : 15 minutes. Local runtime : 20 minutes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8674/8674 [00:00<00:00, 851624.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment took 1163.829 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                name     RR@10   nDCG@10    AP@100\n0  arguana: BM25 >> gte-base-en-v1.5  0.252108  0.376144  0.262613",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@10</th>\n      <th>AP@100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>arguana: BM25 &gt;&gt; gte-base-en-v1.5</td>\n      <td>0.252108</td>\n      <td>0.376144</td>\n      <td>0.262613</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_utils.experiments_helper import split_dev_test, default_complete_test_pipeline\n",
    "\n",
    "dataset_name = \"arguana\"\n",
    "dataset = pt.get_dataset(\"irds:beir/arguana\")\n",
    "topics = dataset.get_topics()\n",
    "\n",
    "dev_topics, test_topics = split_dev_test(topics, test_size=0.8)\n",
    "\n",
    "time_fct(default_complete_test_pipeline, dataset_name, dataset.get_qrels(), test_topics, q_encoder,\n",
    "         eval_metrics)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Will rerun this cell after reindexing the dense index as there is a problem with some documents not being found( no vectors for...). Is it possible that I made the dense index correctly and the \"irds:beir/scifact/test\" misses some documents that are tested in \"irds:beir/scifact\". The error is also reproduced in the debug.ipynb where it can be observed that using only the sparse index does not cause any error so for that reason I think it is because of the dense one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5183/5183 [00:00<00:00, 1446475.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment took 30.021 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                name     RR@10   nDCG@10    AP@100\n0  scifact: BM25 >> gte-base-en-v1.5  0.669475  0.708775  0.664073",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>RR@10</th>\n      <th>nDCG@10</th>\n      <th>AP@100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>scifact: BM25 &gt;&gt; gte-base-en-v1.5</td>\n      <td>0.669475</td>\n      <td>0.708775</td>\n      <td>0.664073</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_utils.experiments_helper import default_complete_test_pipeline_name\n",
    "\n",
    "dataset_name = \"scifact\"\n",
    "# dev_set_name = \"irds:beir/scifact/train\"\n",
    "dataset_test_name = \"irds:beir/scifact/test\"\n",
    "\n",
    "time_fct(\n",
    "    default_complete_test_pipeline_name, dataset_name, dataset_test_name, q_encoder, eval_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from experiment_utils.experiments_helper import split_dev_test, default_complete_test_pipeline\n",
    "\n",
    "dataset_name = \"arguana\"\n",
    "dataset = pt.get_dataset(\"irds:beir/arguana\")\n",
    "topics = dataset.get_topics()\n",
    "\n",
    "# dev_topics, test_topics = split_dev_test(topics, test_size=0.8)\n",
    "\n",
    "time_fct(default_complete_test_pipeline, dataset_name, dataset.get_qrels(), test_topics, q_encoder,\n",
    "         eval_metrics)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "0.10299960100019234"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "timeit.timeit('\"-\".join(str(n) for n in range(100))', number=10000)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
